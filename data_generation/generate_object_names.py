import tensorflow as tf
import argparse
import json
import torch
import os

from helpers.molmo_objs import get_obj_names

from octo.data.oxe import make_oxe_dataset_kwargs
from octo.data.dataset import make_single_dataset

TRAJS_TO_PROCESS = 5200
WRITE_INTERVAL=500

parser = argparse.ArgumentParser()
parser.add_argument("--id", type=int)
parser.add_argument("--dataset", type=str)
parser.add_argument("--size", type=int, default=16)
args = parser.parse_args()

assert args.id < args.size

JSON_PATH = f"/global/scratch/users/riadoshi/vla/generated_data/{args.dataset}/jsons/object_names/batch{args.id}.json"

dtype = torch.bfloat16

tf.config.set_visible_devices(
    [], device_type="gpu"
)
tf.config.set_visible_devices(
    [], device_type="GPU"
)
# load chunk of dataset in based on ID
# assign chunks of 3,500 trajectories per GPU. there are approx 26k trajectories in bridge. so if we have 8 pairs of gpus, then we can assign 3500 and get em all done
print("Loading dataset *******")
start_idx = TRAJS_TO_PROCESS*args.id
dataset_kwargs = make_oxe_dataset_kwargs(args.dataset, "/global/scratch/users/riadoshi/data/")
dataset = make_single_dataset(
                                dataset_kwargs, 
                                frame_transform_kwargs=dict(
                                    resize_size={"primary": (256, 256)},
                                ),
                                train=True)
dataset = dataset.skip(start_idx).take(TRAJS_TO_PROCESS)
iterator = dataset.iterator()


results_dict = {}

# if final json path already exists, load it in
if os.path.exists(JSON_PATH):
    with open(JSON_PATH, "r") as file:
        results_dict = json.load(file)

for i, episode in enumerate(iterator):
    traj_id = int(episode['traj_idx'][0])

    # skip traj if we've already processed it
    if str(traj_id) in results_dict:
        continue
   
    images = episode['observation']['image_primary'].squeeze()
    language_label = episode['task']['language_instruction'][0].decode()

    if language_label != "":
        print("Querying Molmo MoE for object names ******")
        object_names = get_obj_names(
                img=images[0], 
                lang=language_label
        )
    else:
        object_names = []

    print("Saving everything down **************")
    # save down object names & coords generated by MOLMO
    results_dict[traj_id] = object_names

    if i%WRITE_INTERVAL==0:
        with open(f"{JSON_PATH}", "w") as f:
            json.dump(results_dict, f)


with open(f"{JSON_PATH}", "w") as f:
    json.dump(results_dict, f)
    

###########################################
# molmo batched version
    # batch_ids = []
    # batch_images = []
    # batch_language_labels = []
    # for batch_idx in range(BATCH_SIZE):
    #     try:
    #         episode = next(iterator)
    #         traj_id = int(episode['traj_idx'][0])

    #         # save down the first traj id so u know where u are
    #         if batch_idx == 0:
    #             first_traj_id = traj_id

    #         language_label = episode['task']['language_instruction'][0].decode()
    #         img = episode['observation']['image_primary'][0].squeeze()

    #         # skip traj if we've alr processed it
    #         if traj_id in results_dict or language_label == "":
    #             continue

    #         batch_ids.append(traj_id)
    #         batch_images.append(img)
    #         batch_language_labels.append(language_label)
    #     except StopIteration:
    #         break

    
    # object_names = get_obj_names_batched(
    #         imgs=batch_images, 
    #         langs=batch_language_labels
    # )

    # for id in range(first_traj_id, first_traj_id+len(batch_ids)):
    #     if id in batch_ids:
    #         results_dict[id] = obj_names
    #     else:
    #         results_dict[id] = []

    # if i%WRITE_INTERVAL==0:
    #     with open(f"{JSON_PATH}", "w") as f:
    #         json.dump(results_dict, f)   

    


