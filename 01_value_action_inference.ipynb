{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax\n",
    "import cv2\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import mediapy\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from absl import app, flags\n",
    "import optax\n",
    "import json\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import orbax.checkpoint as ocp\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "# from palivla.dataset import prepare_image\n",
    "from palivla.tokenizer import Tokenizer\n",
    "from palivla.load_model import load_model_params_decode\n",
    "from scalax.sharding import MeshShardingHelper, FSDPShardingRule, PartitionSpec\n",
    "from flax.training.train_state import TrainState\n",
    "from jax.experimental import multihost_utils\n",
    "from ml_collections import config_flags\n",
    "from palivla.train_state import PaliVLATrainState\n",
    "from palivla.types import TrainingBatch, RolloutBatch\n",
    "\n",
    "# export TPU_VISIBLE_DEVICES=0 \n",
    "# export TPU_CHIPS_PER_HOST_BOUNDS=1,1,1 \n",
    "# export TPU_HOST_BOUNDS=1,1,1 \n",
    "# export TPU_MESH_CONTROLLER_ADDRESS=localhost:8476 \n",
    "# export TPU_MESH_CONTROLLER_PORT=8476\n",
    "\n",
    "os.environ['TPU_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TPU_CHIPS_PER_HOST_BOUNDS'] = '1,1,1'\n",
    "os.environ['TPU_HOST_BOUNDS'] = '1,1,1'\n",
    "os.environ['TPU_MESH_CONTROLLER_ADDRESS'] = 'localhost:8476'\n",
    "os.environ['TPU_MESH_CONTROLLER_PORT'] = '8476'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298ac8f-da06-41d5-a4a5-145c3080231e",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "Next, we will load a trajectory from the bridge dataset for testing the model. We will use the publicly available copy in the Open X-Embodiment dataset bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\n",
    "ind = np.random.randint(0, 1000)\n",
    "ds = builder.as_dataset(split=f'train[{ind}:{ind+1}]')\n",
    "\n",
    "# sample episode + resize to 256x256 (default third-person cam resolution)\n",
    "episode = next(iter(ds))\n",
    "steps = list(episode['steps'])\n",
    "images = [cv2.resize(np.array(step['observation']['image']), (224, 224)) for step in steps]\n",
    "\n",
    "# extract goal image & language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = steps[0]['observation']['natural_language_instruction'].numpy().decode()\n",
    "\n",
    "# visualize episode\n",
    "print(f'Instruction: {language_instruction}')\n",
    "mediapy.show_video(images, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# action_mean = np.array(dataset_statistics[dataset_name][\"action\"][\"mean\"])\n",
    "# action_std = np.array(dataset_statistics[dataset_name][\"action\"][\"std\"])\n",
    "# action_mask = np.array(dataset_statistics[dataset_name][\"action\"][\"mask\"])\n",
    "def make_inference_batch(batch):\n",
    "    sensors = {\n",
    "        k: batch[\"observation\"][k][None].numpy()\n",
    "        for k in batch[\"observation\"]\n",
    "        if k in model.model_state.model.modality_mappings and k != \"text\"\n",
    "    }\n",
    "    sensors_mask = {\n",
    "        k: batch[\"observation\"][\"pad_mask_dict\"][k].numpy()\n",
    "        for k in model.model_state.model.modality_mappings\n",
    "        if k != \"text\"\n",
    "    }\n",
    "    return RolloutBatch(\n",
    "            sensor_data=sensors,\n",
    "            sensor_masks=sensors_mask,\n",
    "            prompt=batch[\"tokens\"][None].numpy(),\n",
    "            prompt_mask=batch[\"mask_input\"][None].numpy(),\n",
    "            prompt_ar=np.zeros_like(batch[\"mask_ar\"][None]),\n",
    "        )\n",
    "    \n",
    "\n",
    "# Do inference\n",
    "def do_inference(images, instructions):\n",
    "    data = {\n",
    "        \"observation\": {\"image_primary\": images, \"pad_mask_dict\": {\"image_primary\": tf.ones(len(images), dtype=tf.bool)}},\n",
    "        \"task\": {\"language_instruction\": instructions},\n",
    "    }\n",
    "    language_token_instructions = tokenizer.tokenize_language_instruction(data)\n",
    "    # batch = prepare_image(batch)\n",
    "    batch = tokenizer.prepare_tokens_for_generation(data, language_token_instructions)\n",
    "    batch = batch | data\n",
    "    rollout_batch = make_inference_batch(batch)\n",
    "\n",
    "    out_tokens, value = decode(\n",
    "        rollout_batch, None\n",
    "    )\n",
    "    out_tokens = jax.device_get(multihost_utils.process_allgather(out_tokens))\n",
    "    value = jax.device_get(multihost_utils.process_allgather(value))\n",
    "    decoded_actions = tokenizer.detokenize_action(out_tokens)\n",
    "\n",
    "    # Re-normalize actions using dataset statistics\n",
    "    # decoded_actions = decoded_actions * action_std + action_mean\n",
    "    # decoded_actions = unnormalize_action(decoded_actions, dataset_statistics[dataset_name][\"action\"])\n",
    "    decoded_actions = unnormalize_action_minmax(decoded_actions, dataset_statistics[dataset_name][\"action\"])\n",
    "\n",
    "    return decoded_actions, value\n",
    "\n",
    "def unnormalize_action(action, unnormalization_statistics):\n",
    "    mask = unnormalization_statistics.get(\n",
    "        \"mask\", jnp.ones_like(unnormalization_statistics[\"mean\"], dtype=bool)\n",
    "    )\n",
    "    action = action[..., : len(mask)]\n",
    "    action = jnp.where(\n",
    "        mask,\n",
    "        (action * unnormalization_statistics[\"std\"])\n",
    "        + unnormalization_statistics[\"mean\"],\n",
    "        action,\n",
    "    )\n",
    "    return action\n",
    "\n",
    "def unnormalize_action_minmax(action, unnormalization_statistics):\n",
    "    mask = unnormalization_statistics.get(\n",
    "        \"mask\", jnp.ones_like(unnormalization_statistics[\"mean\"], dtype=bool)\n",
    "    )\n",
    "    action = action[..., : len(mask)]\n",
    "    action = jnp.where(\n",
    "        mask,\n",
    "        (action + 1) / 2 * (unnormalization_statistics[\"p99\"] - unnormalization_statistics[\"p01\"]) + unnormalization_statistics[\"p01\"],\n",
    "        action,\n",
    "    )\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b0294",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_checkpoint_dir = \"gs://rail-tpus-mitsuhiko-central2/logs/test/honest-water-37/\"\n",
    "# resume_from_checkpoint_step = \"gs://rail-tpus-mitsuhiko-central2/logs/test/bright-thunder-2/\"\n",
    "resume_from_checkpoint_step = 80000\n",
    "dataset_name = \"bridge_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb184c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharding\n",
    "mesh = MeshShardingHelper([-1], [\"fsdp\"])\n",
    "\n",
    "model_sharding = FSDPShardingRule(\"fsdp\", fsdp_axis_size=mesh.mesh.shape[\"fsdp\"])\n",
    "data_sharding = PartitionSpec(\"fsdp\")\n",
    "# data_sharding = jax.sharding.SingleDeviceSharding(jax.local_devices()[0])\n",
    "\n",
    "restore_checkpoint_manager = ocp.CheckpointManager(\n",
    "        resume_from_checkpoint_dir,\n",
    "        item_handlers=PaliVLATrainState.get_checkpoint_handlers(),\n",
    ")\n",
    "\n",
    "model = PaliVLATrainState.restore(\n",
    "        checkpoint_manager=restore_checkpoint_manager,\n",
    "        step=resume_from_checkpoint_step,\n",
    "        load_optimizer=False,\n",
    "        mesh=mesh,\n",
    "        model_sharding=model_sharding,\n",
    "        data_sharding=data_sharding,\n",
    "    )\n",
    "tokenizer = model.tokenizer\n",
    "decode = model.decode\n",
    "dataset_statistics = model.dataset_statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ffca5",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Next, we will run inference over the images in the episode using the loaded model. \n",
    "Below we demonstrate setups for both, goal-conditioned and language-conditioned training.\n",
    "Note that we need to feed inputs of the correct temporal window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 1\n",
    "# run inference loop, this model only uses single image observations for bridge\n",
    "# collect predicted and true actions\n",
    "pred_actions, true_actions = [], []\n",
    "pred_values = []\n",
    "for step in tqdm.trange(len(images) - (WINDOW_SIZE - 1)):\n",
    "    input_image = images[step][None]\n",
    "    input_image = tf.cast(input_image, tf.float32) / 127.5 - 1.0\n",
    "    instruction = tf.constant(language_instruction)\n",
    "    \n",
    "    actions, value = do_inference(input_image, instruction)\n",
    "    print(value, actions)\n",
    "    pred_actions.append(actions)\n",
    "    pred_values.append(value)\n",
    "\n",
    "    true_actions.append(np.concatenate(\n",
    "        (\n",
    "            steps[step]['action']['world_vector'], \n",
    "            steps[step]['action']['rotation_delta'], \n",
    "            np.array(steps[step]['action']['open_gripper']).astype(np.float32)[None]\n",
    "        ), axis=-1\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99527e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(pred_values).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2efe039",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12a5e3f7",
   "metadata": {},
   "source": [
    "## Visualize predictions and ground-truth actions\n",
    "\n",
    "Finally, we will visualize the predicted actions in comparison to the groundtruth actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ACTION_DIM_LABELS = ['x', 'y', 'z', 'yaw', 'pitch', 'roll', 'grasp']\n",
    "\n",
    "# build image strip to show above actions\n",
    "img_strip = np.concatenate(np.array(images[::3]), axis=1)\n",
    "\n",
    "# set up plt figure\n",
    "figure_layout = [\n",
    "    ['image'] * len(ACTION_DIM_LABELS),\n",
    "    ACTION_DIM_LABELS,\n",
    "    [\"value\"] * len(ACTION_DIM_LABELS),\n",
    "    [\"value8\"] * len(ACTION_DIM_LABELS),\n",
    "]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig, axs = plt.subplot_mosaic(figure_layout)\n",
    "fig.set_size_inches([45, 10])\n",
    "\n",
    "# plot actions\n",
    "pred_actions = np.array(pred_actions).squeeze()\n",
    "true_actions = np.array(true_actions).squeeze()\n",
    "\n",
    "pred_values = np.array(pred_values).squeeze()\n",
    "for action_dim, action_label in enumerate(ACTION_DIM_LABELS):\n",
    "  # actions have batch, horizon, dim, in this example we just take the first action for simplicity\n",
    "  axs[action_label].plot(pred_actions[:, action_dim], label='predicted action')\n",
    "  axs[action_label].plot(true_actions[:, action_dim], label='ground truth')\n",
    "  axs[action_label].set_title(action_label)\n",
    "  axs[action_label].set_xlabel('Time in one episode')\n",
    "\n",
    "# plot value\n",
    "axs['value'].plot(pred_values[..., -2])\n",
    "axs['value8'].plot(pred_values[..., -1])\n",
    "\n",
    "axs['image'].imshow(img_strip)\n",
    "axs['image'].set_xlabel('Time in one episode (subsampled)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eec716a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
