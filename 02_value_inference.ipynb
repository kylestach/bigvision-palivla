{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7229ce10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import jax\n",
    "import cv2\n",
    "import tensorflow_datasets as tfds\n",
    "import tqdm\n",
    "import mediapy\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.config.set_visible_devices([], \"GPU\")\n",
    "\n",
    "import jax.numpy as jnp\n",
    "from absl import app, flags\n",
    "import optax\n",
    "import json\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import orbax.checkpoint as ocp\n",
    "from tensorflow_text import SentencepieceTokenizer\n",
    "# from palivla.dataset import prepare_image\n",
    "from palivla.tokenizer import Tokenizer\n",
    "from palivla.load_model import load_model_params_decode\n",
    "from scalax.sharding import MeshShardingHelper, FSDPShardingRule, PartitionSpec\n",
    "from flax.training.train_state import TrainState\n",
    "from jax.experimental import multihost_utils\n",
    "from ml_collections import config_flags\n",
    "from palivla.train_state import PaliVLATrainState\n",
    "from palivla.types import TrainingBatch, RolloutBatch\n",
    "\n",
    "# export TPU_VISIBLE_DEVICES=0 \n",
    "# export TPU_CHIPS_PER_HOST_BOUNDS=1,1,1 \n",
    "# export TPU_HOST_BOUNDS=1,1,1 \n",
    "# export TPU_MESH_CONTROLLER_ADDRESS=localhost:8476 \n",
    "# export TPU_MESH_CONTROLLER_PORT=8476\n",
    "\n",
    "os.environ['TPU_VISIBLE_DEVICES'] = '2'\n",
    "os.environ['TPU_CHIPS_PER_HOST_BOUNDS'] = '1,1,1'\n",
    "os.environ['TPU_HOST_BOUNDS'] = '1,1,1'\n",
    "os.environ['TPU_MESH_CONTROLLER_ADDRESS'] = 'localhost:8476'\n",
    "os.environ['TPU_MESH_CONTROLLER_PORT'] = '8476'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c298ac8f-da06-41d5-a4a5-145c3080231e",
   "metadata": {},
   "source": [
    "## Load Datasets\n",
    "Next, we will load a trajectory from the bridge dataset for testing the model. We will use the publicly available copy in the Open X-Embodiment dataset bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392bd127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create RLDS dataset builder\n",
    "builder = tfds.builder_from_directory(builder_dir='gs://gresearch/robotics/bridge/0.1.0/')\n",
    "ind = np.random.randint(0, 1000)\n",
    "ds = builder.as_dataset(split=f'train[{ind}:{ind+1}]')\n",
    "\n",
    "# sample episode + resize to 256x256 (default third-person cam resolution)\n",
    "episode = next(iter(ds))\n",
    "steps = list(episode['steps'])\n",
    "images = [cv2.resize(np.array(step['observation']['image']), (224, 224)) for step in steps]\n",
    "\n",
    "# extract goal image & language instruction\n",
    "goal_image = images[-1]\n",
    "language_instruction = steps[0]['observation']['natural_language_instruction'].numpy().decode()\n",
    "\n",
    "# visualize episode\n",
    "print(f'Instruction: {language_instruction}')\n",
    "mediapy.show_video(images, fps=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdb90a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# action_mean = np.array(dataset_statistics[dataset_name][\"action\"][\"mean\"])\n",
    "# action_std = np.array(dataset_statistics[dataset_name][\"action\"][\"std\"])\n",
    "# action_mask = np.array(dataset_statistics[dataset_name][\"action\"][\"mask\"])\n",
    "def make_inference_batch(batch):\n",
    "        sensors = {\n",
    "            k: batch[\"observation\"][k][None].numpy()\n",
    "            for k in batch[\"observation\"]\n",
    "            if k in model.model_state.model.modality_mappings and k != \"text\"\n",
    "        }\n",
    "        sensors_mask = {\n",
    "            k: batch[\"observation\"][\"pad_mask_dict\"][k].numpy()\n",
    "            for k in model.model_state.model.modality_mappings\n",
    "            if k != \"text\"\n",
    "        }\n",
    "        return TrainingBatch(\n",
    "                sensors=sensors,\n",
    "                sensors_mask=sensors_mask,\n",
    "                sensors_next=None,\n",
    "                sensors_next_mask=None,\n",
    "                actions=batch[\"action\"],\n",
    "                tokens=batch[\"tokens\"].numpy(),\n",
    "                tokens_ar=batch[\"mask_ar\"].numpy(),\n",
    "                tokens_loss=batch.get(\"mask_loss\", None),\n",
    "                tokens_mask=batch[\"mask_input\"].numpy(),\n",
    "                rewards=None,\n",
    "                td_mask=None,\n",
    "                mc_returns=None,\n",
    "                next_actions=None,\n",
    "                next_tokens=None,\n",
    "                gen_start=None,\n",
    "            )\n",
    "\n",
    "# Do inference\n",
    "def do_inference(images, instructions, action):\n",
    "        data = {\n",
    "            \"observation\": {\"image_primary\": images, \"pad_mask_dict\": {\"image_primary\": tf.ones(len(images), dtype=tf.bool)}},\n",
    "            \"task\": {\"language_instruction\": instructions},\n",
    "            \"action\": action,\n",
    "        }\n",
    "        language_token_instructions = tokenizer.tokenize_language_instruction(data)\n",
    "        # batch = prepare_image(batch)\n",
    "        batch = tokenizer.prepare_tokens_for_training(data, language_token_instructions)\n",
    "        batch = batch | data\n",
    "        batch = make_inference_batch(batch)\n",
    "        key = jax.random.PRNGKey(0)\n",
    "        key, key_value = jax.random.split(key)\n",
    "        all_inputs = batch.sensors | {\"text\": batch.tokens[..., :-1][None]}\n",
    "        all_masks = batch.sensors_mask | {\n",
    "            \"text\": jnp.ones_like(batch.tokens[..., :-1][None], dtype=jnp.bool_)\n",
    "        }\n",
    "        logits, info = model.model_state.apply_fn(\n",
    "            {\"params\": model.model_state.params},\n",
    "            all_inputs,\n",
    "            data_masks=all_masks,\n",
    "            text_ar_mask=batch.tokens_ar[..., :-1][None],\n",
    "            train=False,\n",
    "            rngs={\"dropout\": key},\n",
    "        )\n",
    "        values = info[\"values\"]\n",
    "        # qs = get_value(values, batch.tokens[..., 1:][None], tokenizer.config)\n",
    "        qs = get_value(values, batch.tokens[..., :-1][None], tokenizer.config)\n",
    "\n",
    "        return qs\n",
    "\n",
    "def normalize_action(action, unnormalization_statistics):\n",
    "    mask = unnormalization_statistics.get(\n",
    "        \"mask\", jnp.ones_like(unnormalization_statistics[\"mean\"], dtype=bool)\n",
    "    )\n",
    "    action = action[..., : len(mask)]\n",
    "    action = jnp.where(\n",
    "        mask,\n",
    "        2 * (action - unnormalization_statistics[\"p01\"]) / (unnormalization_statistics[\"p99\"] - unnormalization_statistics[\"p01\"]) - 1,\n",
    "        action\n",
    "    )\n",
    "    return action\n",
    "\n",
    "def unnormalize_action_minmax(action, unnormalization_statistics):\n",
    "    mask = unnormalization_statistics.get(\n",
    "        \"mask\", jnp.ones_like(unnormalization_statistics[\"mean\"], dtype=bool)\n",
    "    )\n",
    "    action = action[..., : len(mask)]\n",
    "    action = jnp.where(\n",
    "        mask,\n",
    "        (action + 1) / 2 * (unnormalization_statistics[\"p99\"] - unnormalization_statistics[\"p01\"]) + unnormalization_statistics[\"p01\"],\n",
    "        action,\n",
    "    )\n",
    "\n",
    "    return action\n",
    "\n",
    "def get_value(pred_values, tokens, tokenizer_config: Tokenizer.TokenizerConfig):\n",
    "        value_token_starts = jnp.argmax(tokens == tokenizer_config.end_of_action_token, axis=-1)\n",
    "        _get_values = jax.vmap(\n",
    "            lambda x, i: jax.lax.dynamic_slice(x, (i,), (1,))\n",
    "        )\n",
    "        qs = jax.vmap(_get_values, in_axes=(-1, None), out_axes=-1)(\n",
    "            pred_values, value_token_starts\n",
    "        ).squeeze()\n",
    "        return qs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24b0294",
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_from_checkpoint_dir = \"gs://rail-tpus-mitsuhiko-central2/logs/test/honest-water-37/\"\n",
    "# resume_from_checkpoint_step = \"gs://rail-tpus-mitsuhiko-central2/logs/test/bright-thunder-2/\"\n",
    "resume_from_checkpoint_step = 80000\n",
    "dataset_name = \"bridge_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb184c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sharding\n",
    "mesh = MeshShardingHelper([-1], [\"fsdp\"])\n",
    "\n",
    "model_sharding = FSDPShardingRule(\"fsdp\", fsdp_axis_size=mesh.mesh.shape[\"fsdp\"])\n",
    "data_sharding = PartitionSpec(\"fsdp\")\n",
    "# data_sharding = jax.sharding.SingleDeviceSharding(jax.local_devices()[0])\n",
    "\n",
    "restore_checkpoint_manager = ocp.CheckpointManager(\n",
    "        resume_from_checkpoint_dir,\n",
    "        item_handlers=PaliVLATrainState.get_checkpoint_handlers(),\n",
    ")\n",
    "\n",
    "model = PaliVLATrainState.restore(\n",
    "        checkpoint_manager=restore_checkpoint_manager,\n",
    "        step=resume_from_checkpoint_step,\n",
    "        load_optimizer=False,\n",
    "        mesh=mesh,\n",
    "        model_sharding=model_sharding,\n",
    "        data_sharding=data_sharding,\n",
    "    )\n",
    "tokenizer = model.tokenizer\n",
    "decode = model.decode\n",
    "dataset_statistics = model.dataset_statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ed3bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b37ffca5",
   "metadata": {},
   "source": [
    "## Run Inference\n",
    "\n",
    "Next, we will run inference over the images in the episode using the loaded model. \n",
    "Below we demonstrate setups for both, goal-conditioned and language-conditioned training.\n",
    "Note that we need to feed inputs of the correct temporal window size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d6b20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 1\n",
    "# run inference loop, this model only uses single image observations for bridge\n",
    "# collect predicted and true actions\n",
    "pred_actions, true_actions = [], []\n",
    "pred_values = []\n",
    "for step in tqdm.trange(len(images) - (WINDOW_SIZE - 1)):\n",
    "    input_image = images[step][None]\n",
    "    input_image = tf.cast(input_image, tf.float32) / 127.5 - 1.0\n",
    "    instruction = tf.constant(language_instruction)\n",
    "\n",
    "    true_action = np.concatenate(\n",
    "        (\n",
    "            steps[step]['action']['world_vector'], \n",
    "            steps[step]['action']['rotation_delta'], \n",
    "            np.array(steps[step]['action']['open_gripper']).astype(np.float32)[None]\n",
    "        ), axis=-1\n",
    "    )\n",
    "    action = normalize_action(true_action[None], dataset_statistics[dataset_name][\"action\"])\n",
    "\n",
    "\n",
    "    value = do_inference(input_image, instruction, action[None])\n",
    "    print(value)\n",
    "    pred_values.append(value)\n",
    "\n",
    "    true_actions.append(true_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a5e3f7",
   "metadata": {},
   "source": [
    "## Visualize predictions and ground-truth actions\n",
    "\n",
    "Finally, we will visualize the predicted actions in comparison to the groundtruth actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a79775d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ACTION_DIM_LABELS = ['x', 'y', 'z', 'yaw', 'pitch', 'roll', 'grasp']\n",
    "\n",
    "# build image strip to show above actions\n",
    "img_strip = np.concatenate(np.array(images[::3]), axis=1)\n",
    "\n",
    "# set up plt figure\n",
    "figure_layout = [\n",
    "    ['image'] * len(ACTION_DIM_LABELS),\n",
    "    ACTION_DIM_LABELS,\n",
    "    [\"value\"] * len(ACTION_DIM_LABELS),\n",
    "]\n",
    "plt.rcParams.update({'font.size': 12})\n",
    "fig, axs = plt.subplot_mosaic(figure_layout)\n",
    "fig.set_size_inches([45, 10])\n",
    "\n",
    "# plot actions\n",
    "# pred_actions = np.array(pred_actions).squeeze()\n",
    "true_actions = np.array(true_actions).squeeze()\n",
    "for action_dim, action_label in enumerate(ACTION_DIM_LABELS):\n",
    "  # actions have batch, horizon, dim, in this example we just take the first action for simplicity\n",
    "  # axs[action_label].plot(pred_actions[:, action_dim], label='predicted action')\n",
    "  axs[action_label].plot(true_actions[:, action_dim], label='ground truth')\n",
    "  axs[action_label].set_title(action_label)\n",
    "  axs[action_label].set_xlabel('Time in one episode')\n",
    "\n",
    "# plot value\n",
    "axs['value'].plot(pred_values)\n",
    "\n",
    "axs['image'].imshow(img_strip)\n",
    "axs['image'].set_xlabel('Time in one episode (subsampled)')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff0c723",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "big_vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
